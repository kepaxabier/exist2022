{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiaztertest_exist2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJcWJu4mGmll"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "TASK=2\n",
        "LANG='English'\n",
        "if LANG=='Spanish' and TASK==1:\n",
        "  TRANSFORMERMODEL='Roberta'\n",
        "  MODELTYPE='large'\n",
        "  CLASSIFIER='D'\n",
        "  NCLASES=2\n",
        "  DROPOUT=0.1\n",
        "  MODELPATH='/content/drive/MyDrive/TFG_Exist2022/experimentos/bestmodelfull_task1_run1_es_POS2/20220331-070523'\n",
        "  MODELNAME='task1esmodel.pt'\n",
        "  PREPROCESS='N'\n",
        "  THRESHOLD=0.5\n",
        "  MAX_LEN = 128 \n",
        "  BATCH_SIZE = 8\n",
        "if LANG=='Spanish' and TASK==2:\n",
        "  TRANSFORMERMODEL='Bert'\n",
        "  MODELTYPE='base'\n",
        "  CLASSIFIER='A'\n",
        "  NCLASES=6\n",
        "  DROPOUT=0.1\n",
        "  MODELPATH='/content/drive/MyDrive/TFG_Exist2022/experimentos/bestmodelfull_task2_run1_es_POS12/20220331-130948'\n",
        "  MODELNAME='task2esmodel.pt'\n",
        "  PREPROCESS='N'\n",
        "  THRESHOLD=0.5\n",
        "  MAX_LEN = 200 \n",
        "  BATCH_SIZE = 32\n",
        "if LANG=='English' and TASK==1:\n",
        "  TRANSFORMERMODEL='Roberta'\n",
        "  MODELTYPE='large'\n",
        "  CLASSIFIER='D'\n",
        "  NCLASES=2\n",
        "  DROPOUT=0.1\n",
        "  MODELPATH='/content/drive/MyDrive/TFG_Exist2022/experimentos/bestmodelfull_task1_run1_en_POS12/20220330-101349'\n",
        "  MODELNAME='task1enmodel.pt'\n",
        "  PREPROCESS='N'\n",
        "  THRESHOLD=0.5\n",
        "  MAX_LEN = 128 \n",
        "  BATCH_SIZE = 8\n",
        "if LANG=='English' and TASK==2:\n",
        "  TRANSFORMERMODEL='Roberta'\n",
        "  MODELTYPE='large'\n",
        "  CLASSIFIER='D'\n",
        "  NCLASES=6\n",
        "  DROPOUT=0.1\n",
        "  MODELPATH='/content/drive/MyDrive/TFG_Exist2022/experimentos/bestmodelfull_task2_run1_en_POS12_task1_POS10/20220331-122913'\n",
        "  MODELNAME='task2enmodel.pt'\n",
        "  PREPROCESS='N'\n",
        "  THRESHOLD=0.5\n",
        "  MAX_LEN = 128\n",
        "  BATCH_SIZE = 8\n",
        "\n",
        "def load_checkpoint(load_path, model):\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "\n",
        "def get_tokenizer():\n",
        "  if LANG == 'Spanish':\n",
        "    if TRANSFORMERMODEL=='Bert': #BETO\n",
        "      tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
        "    if TRANSFORMERMODEL == 'Roberta':\n",
        "      tokenizer=AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
        "  if LANG == 'English':\n",
        "    if TRANSFORMERMODEL=='Bert':\n",
        "      if MODELTYPE == 'base': #large, xlarge, xxlarge\n",
        "        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "      if MODELTYPE == 'large': #large, xlarge, xxlarge\n",
        "        tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
        "    if TRANSFORMERMODEL == 'Roberta':\n",
        "      if MODELTYPE == 'base': #large, xlarge, xxlarge\n",
        "        tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "      if MODELTYPE == 'large': #large, xlarge, xxlarge\n",
        "        tokenizer = AutoTokenizer.from_pretrained('roberta-large')\n",
        "  return tokenizer\n",
        "\n",
        "if TRANSFORMERMODEL=='Bert':\n",
        "  class BertClassifier(nn.Module):\n",
        "    def __init__(self, n_classes=2, dropout_rate=0.1):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        H=50\n",
        "        D_out=n_classes\n",
        "        if LANG == 'English':\n",
        "          if MODELTYPE == 'base': #large, xlarge, xxlarge\n",
        "            D_in=768\n",
        "            self.bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "          if MODELTYPE == 'large': #large, xlarge, xxlarge\n",
        "            D_in=1024\n",
        "            self.bert = AutoModel.from_pretrained('bert-large-uncased')\n",
        "        if LANG == 'Spanish':\n",
        "          if MODELTYPE == 'base':\n",
        "            D_in=768\n",
        "            self.bert = AutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
        "          if MODELTYPE == 'basecased': #large, xlarge, xxlarge, basecased\n",
        "            D_in=768\n",
        "            self.bert = AutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
        "        if CLASSIFIER=='A':\n",
        "          self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(H, D_out)\n",
        "          )\n",
        "        if CLASSIFIER=='B':\n",
        "          self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(D_in,D_out),\n",
        "            nn.Sigmoid()\n",
        "          )\n",
        "        if CLASSIFIER=='C':\n",
        "          self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(D_in,D_out)\n",
        "          )\n",
        "        if CLASSIFIER=='D':\n",
        "          self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, D_in),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(D_in, D_out)\n",
        "          )      \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits\n",
        "if TRANSFORMERMODEL=='Roberta':\n",
        "  class ROBERTAClassifier(nn.Module):\n",
        "    def __init__(self, n_classes=2, dropout_rate=0.1):\n",
        "        super(ROBERTAClassifier, self).__init__()\n",
        "        H=50\n",
        "        D_out=n_classes\n",
        "        if LANG == 'English':\n",
        "          if MODELTYPE == 'base': #large, xlarge, xxlarge\n",
        "            D_in=768\n",
        "            self.roberta = AutoModel.from_pretrained('roberta-base') #,return_dict=False)\n",
        "          if MODELTYPE == 'large': #large, xlarge, xxlarge\n",
        "            D_in=1024\n",
        "            self.roberta = AutoModel.from_pretrained('roberta-large') #,return_dict=False)\n",
        "        if LANG == 'Spanish':\n",
        "          if MODELTYPE == 'base': #large, xlarge, xxlarge\n",
        "            D_in=768\n",
        "            self.roberta = AutoModel.from_pretrained('xlm-roberta-base')\n",
        "          if MODELTYPE == 'large': #large, xlarge, xxlarge\n",
        "            D_in=1024\n",
        "            self.roberta = AutoModel.from_pretrained('xlm-roberta-large')\n",
        "        if CLASSIFIER=='A':\n",
        "          self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(H, D_out)\n",
        "          )\n",
        "        if CLASSIFIER=='D':\n",
        "          self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, D_in),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(D_in, D_out)\n",
        "          )\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Feed input to BERT\n",
        "        outputs = self.roberta(input_ids=input_ids,attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits\n",
        "\n",
        "#################\n",
        "#Set up GPU/CPU\n",
        "#################\n",
        "#Al igual que transfieres un Tensor a la GPU, transfieres la red neuronal a la GPU.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"cpu o gpu?:\", device)\n",
        "# Clearing Cache space for a fresh Model run\n",
        "torch.cuda.empty_cache() \n",
        "\n",
        "\n",
        "\n",
        "#Instanciamos el modelo\n",
        "if TRANSFORMERMODEL=='Bert':\n",
        "  # Instantiate Bert Classifier\n",
        "  model = BertClassifier(NCLASES,DROPOUT)\n",
        "if TRANSFORMERMODEL=='Roberta':\n",
        "  model=ROBERTAClassifier(NCLASES,DROPOUT)\n",
        "#Instanciamos el tokenizador\n",
        "tokenizer = get_tokenizer()\n",
        "#Cargamos el modelo en la instancia\n",
        "load_checkpoint(MODELPATH + '/' + MODELNAME, model)\n",
        "#Ejecutamos el modelo en la GPU\n",
        "model.to(device)\n",
        "#non-sexist\n",
        "sent=\"This is a beautitul life\"\n",
        "encoded_sent = tokenizer.encode_plus(\n",
        "            text=sent,  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            #return_token_type_ids=False, #return the token type IDs according to the specific tokenizer’s default\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            return_attention_mask=True,      # Return attention mask\n",
        "            return_tensors='pt',           # Return PyTorch tensor\n",
        "            )\n",
        "\n",
        "input_ids = encoded_sent['input_ids'].to(device)\n",
        "attention_mask = encoded_sent['attention_mask'].to(device)\n",
        "output = model(input_ids, attention_mask)\n",
        "if NCLASES == 2:\n",
        "  probs = F.softmax(output, dim=1).cpu()\n",
        "  preds = (probs[:, 1] > THRESHOLD, 1, 0)\n",
        "  predictiontensor= preds[0]\n",
        "  prediction=predictiontensor.cpu().numpy()\n",
        "  result=prediction[0]\n",
        "  if result:\n",
        "    print('sexist')\n",
        "  else:\n",
        "    print('non-sexist')\n",
        "else:\n",
        "  _,predicts=torch.max(output, dim=1)\n",
        "  sexual_type=[\"ideological-inequality\",\"misogyny-non-sexual-violence\",\"non-sexist\",\"objectification\",\"sexual-violence\",\"stereotyping-dominance\"]\n",
        "  predictiontensor=predicts[0]\n",
        "  prediction=predictiontensor.cpu().numpy()\n",
        "  result=sexual_type[prediction]\n",
        "  print(result)\n"
      ]
    }
  ]
}